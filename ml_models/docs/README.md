# Custom Voice-to-Notes NLP Model

This project implements a complete **Voice-to-Notes Generator** without using any pretrained ASR or NLP models. It includes custom implementations of both Automatic Speech Recognition (ASR) and text summarization models built from scratch using only basic PyTorch.

## ğŸ¯ Project Overview

The system consists of two main components:

1. **Custom ASR Model**: A CNN + Bi-LSTM architecture for speech-to-text conversion
2. **Custom Summarization Model**: A Transformer-based encoder-decoder for note generation

## ğŸ“ Project Structure

```
ml_models/
â”œâ”€â”€ asr/
â”‚   â”œâ”€â”€ model.py           # Custom ASR model implementation
â”‚   â”œâ”€â”€ train_asr.py       # ASR model training script
â”‚   â””â”€â”€ evaluate_asr.py    # ASR model evaluation script
â”œâ”€â”€ nlp/
â”‚   â”œâ”€â”€ model_summarizer.py # Custom summarization model implementation
â”‚   â”œâ”€â”€ train_nlp.py       # Summarization model training script
â”‚   â””â”€â”€ evaluate_nlp.py    # Summarization model evaluation script
â”œâ”€â”€ evaluation/            # Evaluation results and metrics
â”œâ”€â”€ docs/                  # Documentation files
â””â”€â”€ pipeline_voice_to_notes.py # End-to-end pipeline
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.7+
- PyTorch
- torchaudio
- numpy
- tqdm
- editdistance

Install dependencies:
```bash
pip install torch torchaudio numpy tqdm editdistance
```

### Training the Models

1. **Train the ASR Model**:
```bash
cd ml_models/asr
python train_asr.py
```

2. **Train the Summarization Model**:
```bash
cd ml_models/nlp
python train_nlp.py
```

### Running the Pipeline

Process an audio file through the complete pipeline:
```bash
cd ml_models
python pipeline_voice_to_notes.py
```

## ğŸ§  Model Architectures

### ASR Model
- **Audio Preprocessing**: 16 kHz mono, Mel-Spectrogram extraction
- **Architecture**: 3Ã—CNN layers â†’ 2Ã—Bi-LSTM layers â†’ Fully connected layer
- **Training Method**: CTC Loss
- **Output**: Character-level speech-to-text

### Summarization Model
- **Tokenizer**: Custom subword BPE
- **Encoder**: 4 Transformer layers, 8 Heads, Hidden size 256
- **Decoder**: 4 layers with teacher forcing, dropout 0.1
- **Output**: Generated notes/summaries

## ğŸ“Š Evaluation Metrics

### ASR Evaluation
- **WER** (Word Error Rate)
- **CER** (Character Error Rate)

### NLP Evaluation
- **ROUGE-1**
- **ROUGE-2**
- **ROUGE-L**

## ğŸ“ˆ Results

The models are evaluated and results are stored in the `evaluation/` directory with performance metrics and graphs.

## ğŸ“„ Documentation

See the `docs/` directory for detailed documentation:
- Model explanations
- Research background
- Dataset descriptions
- Evaluation reports

## âš ï¸ Restrictions

This implementation does NOT use:
- Pretrained ASR models (Whisper, wav2vec, HuBERT, etc.)
- Pretrained NLP models (BART, T5, Pegasus, GPT, etc.)
- Only basic PyTorch is used for all implementations

## ğŸ› ï¸ Modular Coding Structure

The code follows a modular structure with separate modules for:
- Model definitions
- Training scripts
- Evaluation scripts
- Data preprocessing
- Utility functions

## ğŸ–¥ï¸ Hardware Support

The models support both CPU and GPU training and inference.